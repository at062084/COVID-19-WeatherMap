
# Add namespace to cloud account (not sure if will be used)
ibmcloud account space-create CWM-SPACE-DE

# Add namespace to k8s cluster
kubectl create  namespace cwmnamespace

# Add namespace to Kubernetes registry
ibmcloud cr  namespace-add cwmregspace

# Error accesing images in new namespace in k8 registry 
kubectl get secrets -n default | grep "icr-io"
kubectl get secrets -n cwmnamespace | grep "icr-io"
kubectl get secrets -n repnamespace | grep "icr-io"

# https://cloud.ibm.com/docs/containers?topic=containers-registry#copy_imagePullSecret
kubectl get secret default-de-icr-io -n default -o yaml | sed 's/default/cwmnamespace/g' | kubectl create -n cwmnamespace -f -
kubectl get secrets -n cwmnamespace | grep "icr-io"

# https://cloud.ibm.com/docs/containers?topic=containers-registry#store_imagePullSecret
kubectl describe serviceaccount default -n cwmnamespace
kubectl patch -n cwmnamespace serviceaccount/default -p '{"imagePullSecrets":[{"name": "all-icr-io"}]}'

# Order logDNA, then follow commands on logDNA service dashboard
kubectl create namespace ibm-observe
kubectl create secret generic logdna-agent-key -n ibm-observe --from-literal=logdna-agent-key=890ee814a0d8e8d376529949a5884aa6
kubectl create -f https://assets.eu-de.logging.cloud.ibm.com/clients/agent-resources.yaml

# dmesg image on cluster:
Linux version 4.15.0-118-generic (buildd@lgw01-amd64-039) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020 (Ubuntu 4.15.0-118.119-generic 4.15.18)
Command line: BOOT_IMAGE=/vmlinuz-4.15.0-118-generic root=UUID=6ddac925-891d-48c1-b0d5-a2feb62d2367 ro fsck.repair=yes earlyprintk nomodeset nofb vga=normal console=tty1 console=ttyS0 elevator=noop cgroup_enable=memor
SMBIOS 2.4 present.
DMI: Xen HVM domU, BIOS 4.7<denied> 12/14/2020
Hypervisor detected: Xen HVM
Xen version 4.7.
Memory: 4007148K/4189812K available (12300K kernel code, 2482K rwdata, 4272K rodata, 2436K init, 2724K bss, 182664K reserved, 0K cma-reserved)
SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=15, Nodes=1
tsc: Detected 1999.985 MHz processor
tsc: Detected 2000.032 MHz TSC
smpboot: CPU0: Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz (family: 0x6, model: 0x3f, stepping: 0x2)
hpet0: 3 comparators, 64-bit 62.500000 MHz counter
tsc: Refined TSC clocksource calibration: 1999.999 MHz



# 2020-02-01
https://cloud.ibm.com/docs/containers?topic=containers-ingress-types
https://kubernetes.github.io/ingress-nginx/examples/affinity/cookie/
# podID taken from k8s dashboard -> pods -> kube-sytem -> (several restarts) -> public-crbp1g35gf0bfdrpfel34g-alb1-598cb94554-pth8c
# ingress-nginx-controller-873061567-4n3k2 -n kube-system -- cat /etc/nginx/nginx.conf
# ingress-nginx-controller-873061567-4n3k2 -n kube-system -- cat /etc/nginx/ngcin.d/cwmnamespace-cwm-ingress.conf
# root         1     0 TS   19 Jan29 ?        00:05:56 /nginx-ingress -v=3 --nginx-configmaps=kube-system/ibm-cloud-provider-ingress-cm -logtostderr=true
nginx version: nginx/1.17.7

Info on ingress:
nginx-ingress
Image registry.eu-de.bluemix.net/armada-master/ingress:2424
ALB_ID public-crbp1g35gf0bfdrpfel34g-alb1
ALB_ID_LB public-crbp1g35gf0bfdrpfel34g-alb1
ARMADA_CLUSTER_ID bp1g35gf0bfdrpfel34g
ARMADA_POD_NAME public-crbp1g35gf0bfdrpfel34g-alb1-598cb94554-pth8c
INGRESS_IMAGE registry.eu-de.bluemix.net/armada-master/ingress:2424
INGRESS_RESOURCE_CREATION_RATE 3 bytes
INGRESS_RESOURCE_TIMEOUT 2 bytes
SECURED_NAMESPACE ibm-cert-store
Arguments
-v=3
--nginx-configmaps=kube-system/ibm-cloud-provider-ingress-cm
-logtostderr=true

sysctl
Image registry.eu-de.bluemix.net/armada-master/alpine:3.6
Commands
sh
-c
sysctl -e -w fs.file-max=6000000;  sysctl -e -w fs.nr_open=10000000;  sysctl -e -w net.core.rmem_max=16777216;  sysctl -e -w net.core.wmem_max=16777216;  sysctl -e -w net.core.rmem_default=12582912;  sysctl -e -w net.core.wmem_default=12582912;  sysctl -e -w net.core.optmem_max=25165824;  sysctl -e -w net.core.netdev_max_backlog=262144;  sysctl -e -w net.core.somaxconn=32768;  sysctl -e -w net.core.rps_sock_flow_entries=32768;  sysctl -e -w net.ipv4.ip_local_port_range="1025 65535";  sysctl -e -w net.ipv4.tcp_rmem="8192 262144 16777216";  sysctl -e -w net.ipv4.tcp_wmem="8192 262144 16777216";  sysctl -e -w net.ipv4.udp_rmem_min=16384;  sysctl -e -w net.ipv4.udp_wmem_min=16384;  sysctl -e -w net.ipv4.ip_no_pmtu_disc=0;  sysctl -e -w net.ipv4.route.flush=1;  sysctl -e -w net.ipv4.tcp_dsack=1;  sysctl -e -w net.ipv4.tcp_sack=1;  sysctl -e -w net.ipv4.tcp_fack=1;  sysctl -e -w net.ipv4.tcp_max_tw_buckets=1440000;  sysctl -e -w net.ipv4.tcp_tw_recycle=0;  sysctl -e -w net.ipv4.tcp_tw_reuse=1;  sysctl -e -w net.ipv4.tcp_frto=0;  sysctl -e -w net.ipv4.tcp_syncookies=1;  sysctl -e -w net.ipv4.tcp_max_syn_backlog=32768;  sysctl -e -w net.ipv4.tcp_synack_retries=2;  sysctl -e -w net.ipv4.tcp_syn_retries=3;  sysctl -e -w net.ipv4.tcp_fin_timeout=5;  sysctl -e -w net.ipv4.tcp_retries2=5;  sysctl -e -w net.ipv4.tcp_no_metrics_save=1;  sysctl -e -w net.ipv4.tcp_moderate_rcvbuf=1;  sysctl -e -w net.ipv4.tcp_timestamps=1;  sysctl -e -w net.ipv4.tcp_keepalive_time=300;  sysctl -e -w net.ipv4.tcp_keepalive_intvl=30;  sysctl -e -w net.ipv4.tcp_keepalive_probes=6;  sysctl -e -w net.ipv4.tcp_slow_start_after_idle=0;  sysctl -e -w net.ipv4.tcp_window_scaling=1;  sysctl -e -w net.ipv4.tcp_low_latency=1;  sysctl -e -w net.ipv4.tcp_max_orphans=262144;  sysctl -e -w net.nf_conntrack_max=9145728;  sysctl -e -w net.netfilter.nf_conntrack_max=9145728;  sysctl -e -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10;  sysctl -e -w net.netfilter.nf_conntrack_tcp_timeout_fin_wait=10;  sysctl -e -w net.netfilter.nf_conntrack_tcp_timeout_close_wait=30;  sysctl -e -w net.netfilter.nf_conntrack_tcp_loose=1;  sysctl -e -w net.ipv4.tcp_rfc1337=1; exit 0;


/etc/nginx/ngcin.d/cwmnamespace-cwm-ingress.conf
# cat cwmnamespace-cwm-ingress.conf | awk 'NF>0'
  upstream cwmnamespace-cwm-ingress-rep-k8s-cluster.eu-de.containers.appdomain.cloud-cwm-rshiny-service {
    server 172.30.126.41:3838  ;
    keepalive 64;
  }
  server {
    listen 80;
        listen 443 ssl;
        ssl_certificate /etc/nginx/ssl/cwmnamespace-rep-k8s-cluster-48a40589c62cd829903b0d7849def5a2-0000.pem;
        ssl_certificate_key /etc/nginx/ssl/cwmnamespace-rep-k8s-cluster-48a40589c62cd829903b0d7849def5a2-0000.pem;
  server_name rep-k8s-cluster.eu-de.containers.appdomain.cloud;
    if ($http_x_forwarded_proto = 'http') {
      return 301 https://$host$request_uri;
    }
    if ($scheme = http) {
      return 301 https://$host$request_uri;
    }
        location  / {
              proxy_http_version 1.1;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection $connection_upgrade;
              proxy_set_header "x-global-k8fdic-transaction-id" $request_id;
              more_clear_headers "x-global-k8fdic-transaction-id";
              proxy_connect_timeout 60s;
              proxy_read_timeout 60s;
              client_max_body_size 1m;
                proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Host $host;
              proxy_set_header X-Forwarded-Port $server_port;
              proxy_set_header X-Forwarded-Proto https;
              proxy_buffering on;
              proxy_pass http://cwmnamespace-cwm-ingress-rep-k8s-cluster.eu-de.containers.appdomain.cloud-cwm-rshiny-service;
        }
  }




https://nextstrain.org/community/narratives/bergthalerlab/SARSCoV2/Generalaudience3/de
Error fetching one of the datasets. Using the YAML-defined dataset (/community/jgenger/SARS-CoV-2-project/NextstrainAustriav19wIDs) instead.
Could not fetch dataset "//nextstrain.org/community/jgenger/SARS-CoV-2-project/NextstrainAustriav19wIDs". Make sure this dataset exists and is spelled correctly. Error details: status: undefined; message: The requested URL does not exist. (Not Found)

#----------------
Apache: mod_proxy
https://stackoverflow.com/questions/27526281/websockets-and-apache-proxy-how-to-configure-mod-proxy-wstunnel
https://www.serverlab.ca/tutorials/linux/web-servers-linux/how-to-reverse-proxy-websockets-with-apache-2-4/
https://cwiki.apache.org/confluence/display/HTTPD/PerformanceScalingOut#PerformanceScalingOut-LoadBalancingwithApache2.2
#----------------
#
# On ubunt 16+ run a2enmod to enable a module (alternative: use LoadModule in conifg file)
a2enmod proxy rewrite proxy_http proxy_wstunnel

Explicitly configured workers come in two flavors: direct workers and (load) balancer workers. They support many important configuration attributes which are described below in the ProxyPass directive. The same attributes can also be set using ProxySet.

The set of options available for a direct worker depends on the protocol which is specified in the origin server URL. Available protocols include ajp, fcgi, ftp, http and scgi.

Balancer workers are virtual workers that use direct workers known as their members to actually handle the requests. Each balancer can have multiple members. When it handles a request, it chooses a member based on the configured load balancing algorithm.



